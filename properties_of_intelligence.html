<!DOCTYPE html>
<html lang="en">
<head><meta charset="utf-8">
	<title>Research Vision</title>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<link href="style.css" rel="stylesheet">
</head>
<link rel="shortcut icon" type="image/png" href="mythos-scientific.png">
<body style="background-color:#000116; color:#9AA5AF">

<body>

  <!-- Header or Title for the Research Vision -->
  <header>
    <h1>Research Vision</h1>
  </header>

  <!-- Introduction Paragraph -->
  <section id="introduction">
    <p>
      At Mythos Scientific, we observe that a great challenge in AI systems is to achieve open-ended learning: the ability for an artificial neural network to adapt to novel circumstances. To achieve this, we focus on developing self-organization, growth, reproduction, and evolution to occur in our learning algorithms for neural networks. We believe these particular properties will allow for antifragility, allowing for growth from novel stimuli and uncertainty. 
    </p>
  </section>

  <!-- Section 1 -->
  <section id="section-1">
    <h2>Self-organization</h2>
    <p>
Our first step is to create neural networks that self-organize to a desired useful state, e.g. to perform well on a particular task like image recognition or language modeling. With this goal in mind, we have developed an algorithm to update the weights of a neural network, where the individual weight updates are dependent only on local messages that come from the weight's direct neighbors.
    </p>
  </section>

  <!-- Section 2 -->
  <section id="section-2">
    <h2>Growth</h2>
    <p>
Once a self-organized neural network algorithm is able to induce a working set of weights for a wide range of architectures, it allows for changing the architecture of the network on the fly. Such changes can be made also using local messages, allowing for the growth and pruning of the neural network in a similarly self-organized way to the weight updates. 
    </p>
  </section>

  <!-- Section 3 -->
  <section id="section-3">
    <h2>Reproduction</h2>
    <p>
When we achieve growth in a neural network, we have self-organized adaptation of an architecture to a particular task. There are many architecture states from which we grow, and many states towards which we can grow. When the states from which we grow include a "seed" architecture which could be broken off from a larger architecture, we can choose to replicate from that seed state. Self-replication can be thought of a specific case of growth, from the perspective of a particular scale (in this case, the whole-network level).
    </p>
  </section>

  <!-- Section 4 -->
  <section id="section-4">
    <h2>Evolution</h2>
    <p>
If we allow the seed state to include different starting conditions, say different hidden states, or other properties like differences in starting architecture, we can create new networks of starting conditions different from the parent networks. There may be ways of enforcing the initial seed states of a parent network to be similar to, but not the exact same as, its children. If this property of similarity to parents with some amount of random difference is present, we have the ingredients for evolution. If we induce competition and cooperation among different networks, through designing an environment that chooses survivors based on some minimum criteria, we can evolve towards "solving" a particular environment.  
    </p>
  </section>

  <section id="pillars">
     <h2> Pillars of Intelligence </h2>
     <p>
     Jeff Clune, a scientist focused on open-ended learning, has described three pillars of general intelligence: 
     <ul>
	<li> meta-learning the architectures of AI
	<li> meta-learning the learning algorithms themselves, and
	<li> automatically and continually generating new learning environments
     </ul>
     These pillars would be what compose a so-called <b>AI-generating algorithm (AI-GA)</b>. 
     <br>
     <br>Within our framework, the meta-learning learning algorithms come first. We restrict our search through possible learning algorithms to those that have local dependencies only. This allows for new algorithms for growth, which allows us to meta-learn the architectures of neural networks for different tasks. The tasks themselves arise from open-ended environments that allow for evolution to happen with other constraints and possibilities through competition and cooperation for resources in a shared, ever-changing and increasingly complex environment. In other words, we want to create the conditions that would allow for the kinds of problem-solving ability to arise, in a similar way to how living intelligence arose. 
     </p>

</body>
</html>

